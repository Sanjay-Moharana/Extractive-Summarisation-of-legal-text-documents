{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofReadFile = []\n",
    "# reading the name of legal documents\n",
    "for file in os.listdir(\"LegalCorpus\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        listofReadFile.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n",
      "1 : 1953_24.txt  2 : 1953_28.txt  "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'D.' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-123b7676ef2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mbagOfWords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatizedWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mdel\u001b[0m \u001b[0mlistofParagraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlistofParagraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[0mlemmatizedWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'D.' is not in list"
     ]
    }
   ],
   "source": [
    "dictionaryParagraphCorpus = {}\n",
    "dictionaryParagraphWordCorpus = {}\n",
    "\n",
    "countFile = 1\n",
    "\n",
    "print('Building...')\n",
    "for fileName in listofReadFile:\n",
    "    #reading the document\n",
    "    file = open('LegalCorpus/'+fileName, 'r', encoding=\"utf8\")\n",
    "\n",
    "    #reading the contents of each document as string\n",
    "    listofParagraph = []\n",
    "    listofParagraph = (file.read()).strip().split(\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    # formatting the sentence of a document\n",
    "    temp = []\n",
    "    for eachLine in listofParagraph[:]:\n",
    "        eachLine = re.sub(r'(\\d\\d\\d|\\d\\d|\\d)\\.\\s', ' ', eachLine)#removes the paragraph lables 1. or 2. etc.\n",
    "        eachLine = re.sub(r'(?<=[a-zA-Z])\\.(?=\\d)', '', eachLine)#removes dot(.) i.e File No.1063\n",
    "        eachLine = re.sub(r'(?<=\\d|[a-zA-Z])\\.(?=\\s[\\da-z])', ' ', eachLine)#to remove the ending dot of abbr\n",
    "        eachLine = re.sub(r'(?<=\\d|[a-zA-Z])\\.(?=\\s?[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~])', '', eachLine)#to remove the ending dot of abbr\n",
    "        temp.append(eachLine.strip())\n",
    "    listofParagraph = []\n",
    "    listofParagraph = temp\n",
    "\n",
    "    # Finding stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tempPara = []\n",
    "    tempPara = listofParagraph.copy()\n",
    "    listofParagraph = []\n",
    "    paraGraphBagOfWord = []\n",
    "    #Tokenising the contents of each document\n",
    "    for eachParagraph in tempPara[:]:\n",
    "        #Tokenising the document into sentences\n",
    "        tokenisedSentences = []\n",
    "        tokenisedSentences = nltk.sent_tokenize(eachParagraph)\n",
    "        temp = []\n",
    "        temp = tokenisedSentences.copy()\n",
    "        bagOfWords = []\n",
    "        lemmatizedWords = []\n",
    "\n",
    "        line = \"\"\n",
    "        for eachLine in temp[:]:\n",
    "            line = eachLine\n",
    "            eachLine = re.sub(r'[\\.\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', '', eachLine)\n",
    "            tokenisedWords = nltk.word_tokenize(eachLine.lower())\n",
    "\n",
    "            # removing the stopwords from a sentence\n",
    "            withoutStopwords = [eachWord for eachWord in tokenisedWords if not eachWord in stop_words]\n",
    "\n",
    "            # removing the punctuation from a sentence\n",
    "            withoutStopwords = [eachWord for eachWord in withoutStopwords if not eachWord in string.punctuation]\n",
    "\n",
    "            lemmatizer = nltk.WordNetLemmatizer()\n",
    "            for words, tags in nltk.tag.pos_tag(withoutStopwords):\n",
    "                if(tags.startswith('JJ')):\n",
    "                    lemmatizedWords.append(lemmatizer.lemmatize(words, pos='a'))\n",
    "                elif(tags.startswith('VB')):\n",
    "                    lemmatizedWords.append(lemmatizer.lemmatize(words, pos='v'))\n",
    "                elif(tags.startswith('RB')):\n",
    "                    lemmatizedWords.append(lemmatizer.lemmatize(words, pos='r'))\n",
    "                elif(tags.startswith('NN')):\n",
    "                    lemmatizedWords.append(lemmatizer.lemmatize(words, pos='n'))\n",
    "                else:\n",
    "                    lemmatizedWords.append(lemmatizer.lemmatize(words))\n",
    "\n",
    "            if(lemmatizedWords != []):\n",
    "                bagOfWords.append(lemmatizedWords)\n",
    "            else:\n",
    "                del tokenisedSentences[tokenisedSentences.index(line)]\n",
    "            lemmatizedWords = []\n",
    "        \n",
    "        #storing the paragraphs with sentence in each para : list[paragraph[sent]]\n",
    "        listofParagraph.append(tokenisedSentences)\n",
    "        #storing bag of words for paragraph : list[paragraph[tokenised words of sent]]\n",
    "        paraGraphBagOfWord.append(bagOfWords)\n",
    "            \n",
    "    # Save words\n",
    "    dictionaryParagraphWordCorpus[fileName] = paraGraphBagOfWord\n",
    "    \n",
    "    # Paragraph Corpus\n",
    "    totalWordsinDocument = 0\n",
    "    for eachPara in listofParagraph[:]:\n",
    "        for eachSent in eachPara[:]:\n",
    "            sent = eachSent.strip().split(' ')\n",
    "            totalWordsinDocument += len(sent)\n",
    "    totalWordsinDocument = math.ceil(totalWordsinDocument * 0.33)\n",
    "\n",
    "    # Save sentences\n",
    "    dictionaryParagraphCorpus[fileName] = [listofParagraph, totalWordsinDocument]\n",
    "    \n",
    "    # visuals\n",
    "    print (countFile, '>', fileName, end = '  ')\n",
    "    if(countFile % 10 == 0):\n",
    "        print('\\n')\n",
    "    countFile += 1\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving dictionary of paragraph to .json file\n",
    "with open('ParagraphCorpus.json', 'w') as fp:\n",
    "    json.dump(dictionaryParagraphCorpus, fp, sort_keys=True, indent=0)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving dictionary of paragraph to .json file\n",
    "with open('ParagraphWordCorpus.json', 'w') as fp:\n",
    "    json.dump(dictionaryParagraphWordCorpus, fp, sort_keys=True, indent=0)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
