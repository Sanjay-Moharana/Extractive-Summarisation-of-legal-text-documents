{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import json\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofReadFile = []\n",
    "# reading the name of legal documents\n",
    "for file in os.listdir(\"LegalCorpus\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        listofReadFile.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fun to compute the term frequency TF(t, d) = 1 + log(tf)\n",
    "def computeTermFrequency(dictionary):\n",
    "    for term, count in dictionary.items():\n",
    "        dictionary[term] = 1 + math.log10(count)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionarySentCorpus = {}\n",
    "dictionaryWordCorpus = {}\n",
    "\n",
    "dictionaryUniqueBowOfDoc = {}\n",
    "dictionaryTFofDoc = {}\n",
    "\n",
    "countFile = 1\n",
    "\n",
    "print('Building...')\n",
    "for fileName in listofReadFile:\n",
    "    #reading the document\n",
    "    file = open('LegalCorpus/'+fileName, 'r', encoding=\"utf8\")\n",
    "\n",
    "    #reading the contents of each document as string\n",
    "    listofSentences = []\n",
    "    listofSentences = (file.read()).strip().split(\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    # formatting the sentence of a document\n",
    "    temp = \"\"\n",
    "    for eachLine in listofSentences[:]:\n",
    "        eachLine = re.sub(r'(\\d\\d\\d|\\d\\d|\\d)\\.\\s', ' ', eachLine)#removes the paragraph lables 1. or 2. etc.\n",
    "        eachLine = re.sub(r'(?<=[a-zA-Z])\\.(?=\\d)', '', eachLine)#removes dot(.) i.e File No.1063\n",
    "        eachLine = re.sub(r'(?<=\\d|[a-zA-Z])\\.(?=\\s[\\da-z])', ' ', eachLine)#to remove the ending dot of abbr\n",
    "        eachLine = re.sub(r'(?<=\\d|[a-zA-Z])\\.(?=\\s?[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~])', '', eachLine)#to remove the ending dot of abbr\n",
    "        temp = temp +' '+eachLine\n",
    "    listofSentences = []\n",
    "    listofSentences = temp\n",
    "\n",
    "    #Tokenising the document into sentences\n",
    "    tokenisedSentences = []\n",
    "    tokenisedSentences = nltk.sent_tokenize(listofSentences)\n",
    "\n",
    "    sent = []\n",
    "    for eachLine in tokenisedSentences[:]:\n",
    "        eachLine = eachLine.strip()\n",
    "        sent.append(eachLine)\n",
    "    tokenisedSentences = []\n",
    "    tokenisedSentences = sent\n",
    "\n",
    "    bagOfWords = []\n",
    "    lemmatizedWords = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    temp = []\n",
    "    temp = tokenisedSentences.copy()\n",
    "    index = 0\n",
    "    line = \"\"\n",
    "    unionOfBoW = []\n",
    "    #Tokenising the contents of each document\n",
    "    for eachLine in temp[:]:\n",
    "        line = eachLine\n",
    "        eachLine = re.sub(r'[\\.\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', '', eachLine)\n",
    "        tokenisedWords = nltk.word_tokenize(eachLine.lower())\n",
    "\n",
    "        # removing the stopwords from a sentence\n",
    "        withoutStopwords = [eachWord for eachWord in tokenisedWords if not eachWord in stop_words]\n",
    "\n",
    "        # removing the punctuation from a sentence\n",
    "        withoutStopwords = [eachWord for eachWord in withoutStopwords if not eachWord in string.punctuation]\n",
    "\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "        for words, tags in nltk.tag.pos_tag(withoutStopwords):\n",
    "            if(tags.startswith('JJ')):\n",
    "                lemmatizedWords.append(lemmatizer.lemmatize(words, pos='a'))\n",
    "                words = lemmatizer.lemmatize(words, pos='a')\n",
    "            elif(tags.startswith('VB')):\n",
    "                lemmatizedWords.append(lemmatizer.lemmatize(words, pos='v'))\n",
    "                words = lemmatizer.lemmatize(words, pos='v')\n",
    "            elif(tags.startswith('RB')):\n",
    "                lemmatizedWords.append(lemmatizer.lemmatize(words, pos='r'))\n",
    "                words = lemmatizer.lemmatize(words, pos='r')\n",
    "            elif(tags.startswith('NN')):\n",
    "                lemmatizedWords.append(lemmatizer.lemmatize(words, pos='n'))\n",
    "                words = lemmatizer.lemmatize(words, pos='n')\n",
    "            else:\n",
    "                lemmatizedWords.append(lemmatizer.lemmatize(words))\n",
    "                words = lemmatizer.lemmatize(words)\n",
    "\n",
    "        if(lemmatizedWords != []):\n",
    "            bagOfWords.append(lemmatizedWords)\n",
    "            unionOfBoW = set(unionOfBoW).union(set(lemmatizedWords))\n",
    "        else:\n",
    "            del tokenisedSentences[tokenisedSentences.index(line)]\n",
    "        lemmatizedWords = []\n",
    "        index += 1\n",
    "\n",
    "    # Save words\n",
    "    dictionaryWordCorpus[fileName] = bagOfWords\n",
    "\n",
    "    totalWordsinDocument = 0\n",
    "    for eachSent in tokenisedSentences[:]:\n",
    "        sent = eachSent.strip().split(' ')\n",
    "        totalWordsinDocument += len(sent)\n",
    "    totalWordsinDocument = math.ceil(totalWordsinDocument * 0.33)\n",
    "\n",
    "    # Save sentences\n",
    "    dictionarySentCorpus[fileName] = [tokenisedSentences, totalWordsinDocument]\n",
    "\n",
    "    '''computing the term frequency (TF)'''\n",
    "    dictionary = {}\n",
    "    #creating dictionary for storing the score for each term t\n",
    "    dictionary = dict.fromkeys(unionOfBoW, 0)\n",
    "\n",
    "    #counting frequency of each term in the document\n",
    "    for eachKey in dictionary:\n",
    "        for eachBow in bagOfWords:\n",
    "            for word in eachBow:\n",
    "                if(eachKey == word):\n",
    "                    dictionary[eachKey] += 1\n",
    "\n",
    "    #Calculating the weighted term frequency\n",
    "    tfDictionary = {}\n",
    "    tfDictionary = computeTermFrequency(dictionary)\n",
    "\n",
    "    dictionaryTFofDoc[fileName] = tfDictionary\n",
    "\n",
    "    # Save unique words in single list for one document\n",
    "    dictionaryUniqueBowOfDoc[fileName] = list(unionOfBoW)\n",
    "\n",
    "    print (countFile, ':', fileName, end = '  ')\n",
    "    if(countFile % 10 == 0):\n",
    "        print('\\n')\n",
    "    countFile += 1\n",
    "\n",
    "    \n",
    "    # collecting the garbages which takes up memory. Helps to free up memory ==> Execution is Faster.\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving dictionary of sentences to .json file\n",
    "with open('SentCorpus.json', 'w') as fp:\n",
    "    json.dump(dictionarySentCorpus, fp, sort_keys=True, indent=0)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving dictionary of bag of words to .json file\n",
    "with open('BagOfWordCorpus.json', 'w') as fp:\n",
    "    json.dump(dictionaryWordCorpus, fp, sort_keys=True, indent=0)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving dictionary of unique bag of words to .json file \n",
    "with open('UniqueBowOfDocForIDFCorpus.json', 'w') as fp:\n",
    "    json.dump(dictionaryUniqueBowOfDoc, fp, sort_keys=True, indent=0)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving dictionary of unique bag of words to .json file\n",
    "with open('TFCorpus.json', 'w') as fp:\n",
    "    json.dump(dictionaryTFofDoc, fp, sort_keys=True, indent=0)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nBuild Complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
