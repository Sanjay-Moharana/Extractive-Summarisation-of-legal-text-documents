{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the summary corpus\n",
    "with open('SummaryCorpus.json', 'r') as fp:\n",
    "    readSummary = json.load(fp)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the sentences corpus\n",
    "with open('SentCorpus.json', 'r') as fp:\n",
    "    readSent = json.load(fp)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the bag of word corpus\n",
    "with open('BagOfWordCorpus.json', 'r') as fp:\n",
    "    readWord = json.load(fp)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the IDF Corpus\n",
    "with open('IDFCorpus13.json', 'r') as fp:\n",
    "    readIDF13 = json.load(fp)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the IDF Corpus\n",
    "with open('IDFCorpus18.json', 'r') as fp:\n",
    "    readIDF18 = json.load(fp)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9e8a5bbfaf8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Reading the DF Corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TFCorpus.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mreadTF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m     return loads(fp.read(),\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Reading the DF Corpus\n",
    "with open('TFCorpus.json', 'r') as fp:\n",
    "    readTF = json.load(fp)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the legal dictionary\n",
    "with open('LegalDictionaryCorpus.json', 'r') as fp:\n",
    "    legalDictionary = json.load(fp)\n",
    "fp.close()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = []\n",
    "listOfFiles = []\n",
    "listOfDocument = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reading the document\n",
    "fileName = input('Please insert the name of the document : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencesofDocument = readSent[fileName]\n",
    "bagofWordsofDocument = readWord[fileName]\n",
    "dictTF = readTF[fileName]\n",
    "\n",
    "if(fileName.startswith('19') or fileName.startswith('2010') or fileName.startswith('2011') or fileName.startswith('2012') or fileName.startswith('2013')):\n",
    "        dictIDF = readIDF13[fileName]\n",
    "    else:\n",
    "        dictIDF = readIDF18[fileName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the term frequency of each dictionary\n",
    "tfDictionary = {}\n",
    "tfDictionary = dictTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfDictionary = {}\n",
    "idfDictionary = dictIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfDictionary, idfDictionary):\n",
    "    tfidf = {}\n",
    "    found = 0\n",
    "    for term, value in tfDictionary.items():\n",
    "        if value > 0 and term in legalDictionary['LegalDict']:\n",
    "            found = 1\n",
    "\n",
    "        if(found == 1):\n",
    "             tfidf[term] = value * idfDictionary[term] * 2.0 # giving more weightage to legal terms\n",
    "        else:\n",
    "            tfidf[term] = value * idfDictionary[term] / 2.0  # tfidf normal calculation for non legal terms\n",
    "        found = 0\n",
    "\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidfDictionary = {}\n",
    "tfidfDictionary = computeTFIDF(tfDictionary, idfDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computing the length of the each sentence in terms of #words in the sentences\n",
    "countTokensinSentence = []\n",
    "for eachLine in sentencesofDocument[0]:\n",
    "    tokenisedWords = nltk.word_tokenize(eachLine)\n",
    "    if(tokenisedWords != []): # for counting the number of terms\n",
    "        countTokensinSentence.append(tokenisedWords)\n",
    "\n",
    "\n",
    "sentenceScore = []\n",
    "dictSentenceScore = {}\n",
    "sum = 0\n",
    "i = 0\n",
    "for sent in bagofWordsofDocument:\n",
    "    for word in sent:\n",
    "        sum += tfidfDictionary[word]\n",
    "    sum = (sum/(len(countTokensinSentence[i])))\n",
    "    sentenceScore.append(sum)\n",
    "    dictSentenceScore[i] = sum\n",
    "    i += 1\n",
    "    sum = 0\n",
    "    \n",
    "import statistics\n",
    "sd = statistics.stdev(sentenceScore)\n",
    "\n",
    "ner = []\n",
    "listdigits = []\n",
    "named_entities = []\n",
    "from date_extractor import extract_dates\n",
    "for sent in sentencesofDocument[0]:\n",
    "    tokenized_doc = nltk.word_tokenize(sent)\n",
    "    # tag sentences and use nltk's Named Entity Chunker\n",
    "    tagged_sentences = nltk.pos_tag(tokenized_doc)\n",
    "    ne_chunked_sents = nltk.ne_chunk(tagged_sentences)\n",
    "    # extract all named entities\n",
    "    named_entities = []\n",
    "    for tagged_tree in ne_chunked_sents:\n",
    "        if hasattr(tagged_tree, 'label'):\n",
    "            entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #\n",
    "            entity_type = tagged_tree.label() # get NE category\n",
    "            named_entities.append((entity_name, entity_type))\n",
    "    ner.append(len(named_entities))\n",
    "\n",
    "    digits = re.findall(r'\\d+', sent)\n",
    "    listdigits.append(len(digits))\n",
    "    \n",
    "sum = 0\n",
    "l = 0\n",
    "i = 0\n",
    "for sent in bagofWordsofDocument:\n",
    "    for word in sent:\n",
    "        if word in legalDictionary['LegalDict']:\n",
    "            l += 1\n",
    "\n",
    "    d = listdigits[i]\n",
    "    e = ner[i]\n",
    "    sum += sentenceScore[i] + (sd*(1.5*l + 0.2*d + 0.3*e))/len(countTokensinSentence[i])\n",
    "    sentenceScore.append(sum)\n",
    "    dictSentenceScore[i] = sum\n",
    "    i += 1\n",
    "    sum = 0\n",
    "    l = 0\n",
    "\n",
    "\n",
    "sentenceIndex = sorted(dictSentenceScore, key=dictSentenceScore.get, reverse=True)\n",
    "\n",
    "system_generated_summary = \"\"\n",
    "termCount = 0\n",
    "tempIndices = []\n",
    "sent = []\n",
    "totalWordsinDocument = sentencesofDocument[1]\n",
    "for index in sentenceIndex:\n",
    "    if (termCount <= totalWordsinDocument):\n",
    "        tempIndices.append(index)\n",
    "        termCount += len(sentencesofDocument[0][index])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING THE COLORS FOR HEAT MAP\n",
    "from IPython.display import Markdown\n",
    "\n",
    "upper = \"0xff0000\"\n",
    "lower = \"0x0000ff\"\n",
    "totalSent = len(tempIndices)\n",
    "lowSent = totalSent//2\n",
    "topSent = totalSent - lowSent\n",
    "\n",
    "incFactortopRed = (256)//topSent\n",
    "incFactortopGreen = (256)//topSent\n",
    "\n",
    "if(topSent >= lowSent):\n",
    "    loopfor = topSent\n",
    "    \n",
    "upperColors = []\n",
    "lowerColors = []\n",
    "upperColors.append('#ff0000')\n",
    "lowerColors.append('#0000ff')\n",
    "for i in range(1, loopfor):\n",
    "    upper = upper.replace(\"0x\", \"\")\n",
    "    upper = list(upper)\n",
    "    \n",
    "    red = '0x' + upper[0] + upper[1]\n",
    "    green = '0x' + upper[2] + upper[3]\n",
    "    \n",
    "    i = int(green, 16)\n",
    "    i += incFactortopGreen\n",
    "    k = int(red, 16)\n",
    "    k -= incFactortopRed\n",
    "    \n",
    "    i = hex(i)\n",
    "    k = hex(k)\n",
    "    \n",
    "    i = str(i).replace(\"0x\", \"\")\n",
    "    k = str(k).replace(\"0x\", \"\")\n",
    "    \n",
    "    if(len(i)==1):\n",
    "        i = '0'+ i\n",
    "    if(len(k)==1):\n",
    "        k = '0' + k\n",
    "    \n",
    "    upper = '0x' + k + i + '00'\n",
    "    \n",
    "    i = str(upper).replace('0x', '')\n",
    "\n",
    "    j = list(str(i))\n",
    "    j[4] = j[0]\n",
    "    j[5] = j[1]\n",
    "    j[0] = '0'\n",
    "    j[1] = '0'\n",
    "    j = \"\" + \"\".join(j)\n",
    "    \n",
    "    upperColors.append('#' + i)\n",
    "    lowerColors.append('#' + j)\n",
    "\n",
    "if(topSent < lowSent):\n",
    "    del lowerColors[-1]\n",
    "\n",
    "listColor = []\n",
    "listColor = upperColors\n",
    "for i in range(len(lowerColors)):\n",
    "    listColor.append(lowerColors[-i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictColor = {}\n",
    "j = 0\n",
    "for i in tempIndices:\n",
    "    dictColor[i] = listColor[j]\n",
    "    j += 1\n",
    "\n",
    "sentenceIndex = []\n",
    "sentenceIndex = sorted(tempIndices)\n",
    "    \n",
    "# print highest 34% words of the total sentences\n",
    "for index in sentenceIndex:\n",
    "    display (Markdown('<span style=\"color: '+ dictColor[index] +'\">'+str(dictSentenceScore[index])+' '+sentencesofDocument[0][index]+'</span>'))\n",
    "    system_generated_summary += sentencesofDocument[0][index]\n",
    "flag = 0\n",
    "if not len(sentencesofDocument[0])-1 in sentenceIndex:\n",
    "    for key, value in dictColor.items():\n",
    "        if(dictSentenceScore[len(sentencesofDocument[0])-1] >= dictSentenceScore[key]):\n",
    "            flag = 1\n",
    "            break\n",
    "    if(flag == 1):\n",
    "        display (Markdown('<span style=\"color: '+dictColor[key]+'\">'+str(dictSentenceScore[len(sentencesofDocument[0])-1])+' '+sentencesofDocument[0][len(sentencesofDocument[0])-1]+'</span>'))\n",
    "        system_generated_summary += sentencesofDocument[0][len(sentencesofDocument[0])-1]\n",
    "    \n",
    "    else:\n",
    "        display (Markdown('<span style=\"color: #0000ff\">'+str(dictSentenceScore[len(sentencesofDocument[0])-1])+' '+sentencesofDocument[0][len(sentencesofDocument[0])-1]+'</span>'))\n",
    "        system_generated_summary += sentencesofDocument[0][len(sentencesofDocument[0])-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "hypothesis = system_generated_summary\n",
    "\n",
    "reference = readSummary[fileName]\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print ('Rouge-1 : ', scores[0]['rouge-1'])\n",
    "print ('Rouge-2 : ', scores[0]['rouge-2'])\n",
    "print ('Rouge-L : ', scores[0]['rouge-l'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
